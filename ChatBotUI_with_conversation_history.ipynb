{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Mj3V525oz2QYnkVhvtDN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvRpbj_etifQ"
      },
      "outputs": [],
      "source": [
        "import streamlit as st # Import python packages\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "\n",
        "from snowflake.cortex import Complete\n",
        "from snowflake.core import Root\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "pd.set_option(\"max_colwidth\",None)\n",
        "\n",
        "### Default Values\n",
        "NUM_CHUNKS = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n",
        "slide_window = 7 # how many last conversations to remember. This is the slide window.\n",
        "\n",
        "# service parameters\n",
        "CORTEX_SEARCH_DATABASE = \"CC_QUICKSTART_CORTEX_SEARCH_DOCS\"\n",
        "CORTEX_SEARCH_SCHEMA = \"DATA\"\n",
        "CORTEX_SEARCH_SERVICE = \"CC_SEARCH_SERVICE_CS\"\n",
        "######\n",
        "######\n",
        "\n",
        "# columns to query in the service\n",
        "COLUMNS = [\n",
        "    \"chunk\",\n",
        "    \"relative_path\",\n",
        "    \"category\"\n",
        "]\n",
        "\n",
        "session = get_active_session()\n",
        "root = Root(session)\n",
        "\n",
        "svc = root.databases[CORTEX_SEARCH_DATABASE].schemas[CORTEX_SEARCH_SCHEMA].cortex_search_services[CORTEX_SEARCH_SERVICE]\n",
        "\n",
        "### Functions\n",
        "\n",
        "def config_options():\n",
        "\n",
        "    st.sidebar.selectbox('Select your model:',(\n",
        "                                    'mixtral-8x7b',\n",
        "                                    'snowflake-arctic',\n",
        "                                    'mistral-large',\n",
        "                                    'llama3-8b',\n",
        "                                    'llama3-70b',\n",
        "                                    'reka-flash',\n",
        "                                     'mistral-7b',\n",
        "                                     'llama2-70b-chat',\n",
        "                                     'gemma-7b'), key=\"model_name\")\n",
        "\n",
        "    categories = session.table('docs_chunks_table').select('category').distinct().collect()\n",
        "\n",
        "    cat_list = ['ALL']\n",
        "    for cat in categories:\n",
        "        cat_list.append(cat.CATEGORY)\n",
        "\n",
        "    st.sidebar.selectbox('Select what products you are looking for', cat_list, key = \"category_value\")\n",
        "\n",
        "    st.sidebar.checkbox('Do you want that I remember the chat history?', key=\"use_chat_history\", value = True)\n",
        "\n",
        "    st.sidebar.checkbox('Debug: Click to see summary generated of previous conversation', key=\"debug\", value = True)\n",
        "    st.sidebar.button(\"Start Over\", key=\"clear_conversation\", on_click=init_messages)\n",
        "    st.sidebar.expander(\"Session State\").write(st.session_state)\n",
        "\n",
        "def init_messages():\n",
        "\n",
        "    # Initialize chat history\n",
        "    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "def get_similar_chunks_search_service(query):\n",
        "\n",
        "    if st.session_state.category_value == \"ALL\":\n",
        "        response = svc.search(query, COLUMNS, limit=NUM_CHUNKS)\n",
        "    else:\n",
        "        filter_obj = {\"@eq\": {\"category\": st.session_state.category_value} }\n",
        "        response = svc.search(query, COLUMNS, filter=filter_obj, limit=NUM_CHUNKS)\n",
        "\n",
        "    st.sidebar.json(response.json())\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "def get_chat_history():\n",
        "#Get the history from the st.session_stage.messages according to the slide window parameter\n",
        "\n",
        "    chat_history = []\n",
        "\n",
        "    start_index = max(0, len(st.session_state.messages) - slide_window)\n",
        "    for i in range (start_index , len(st.session_state.messages) -1):\n",
        "         chat_history.append(st.session_state.messages[i])\n",
        "\n",
        "    return chat_history\n",
        "\n",
        "def summarize_question_with_history(chat_history, question):\n",
        "# To get the right context, use the LLM to first summarize the previous conversation\n",
        "# This will be used to get embeddings and find similar chunks in the docs for context\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        Based on the chat history below and the question, generate a query that extend the question\n",
        "        with the chat history provided. The query should be in natual language.\n",
        "        Answer with only the query. Do not add any explanation.\n",
        "\n",
        "        <chat_history>\n",
        "        {chat_history}\n",
        "        </chat_history>\n",
        "        <question>\n",
        "        {question}\n",
        "        </question>\n",
        "        \"\"\"\n",
        "\n",
        "    sumary = Complete(st.session_state.model_name, prompt)\n",
        "\n",
        "    if st.session_state.debug:\n",
        "        st.sidebar.text(\"Summary to be used to find similar chunks in the docs:\")\n",
        "        st.sidebar.caption(sumary)\n",
        "\n",
        "    sumary = sumary.replace(\"'\", \"\")\n",
        "\n",
        "    return sumary\n",
        "\n",
        "def create_prompt (myquestion):\n",
        "\n",
        "    if st.session_state.use_chat_history:\n",
        "        chat_history = get_chat_history()\n",
        "\n",
        "        if chat_history != []: #There is chat_history, so not first question\n",
        "            question_summary = summarize_question_with_history(chat_history, myquestion)\n",
        "            prompt_context =  get_similar_chunks_search_service(question_summary)\n",
        "        else:\n",
        "            prompt_context = get_similar_chunks_search_service(myquestion) #First question when using history\n",
        "    else:\n",
        "        prompt_context = get_similar_chunks_search_service(myquestion)\n",
        "        chat_history = \"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "           You are an expert chat assistance that extracs information from the CONTEXT provided\n",
        "           between <context> and </context> tags.\n",
        "           You offer a chat experience considering the information included in the CHAT HISTORY\n",
        "           provided between <chat_history> and </chat_history> tags..\n",
        "           When ansering the question contained between <question> and </question> tags\n",
        "           be concise and do not hallucinate.\n",
        "           If you donÂ´t have the information just say so.\n",
        "\n",
        "           Do not mention the CONTEXT used in your answer.\n",
        "           Do not mention the CHAT HISTORY used in your asnwer.\n",
        "\n",
        "           Only anwer the question if you can extract it from the CONTEXT provideed.\n",
        "\n",
        "           <chat_history>\n",
        "           {chat_history}\n",
        "           </chat_history>\n",
        "           <context>\n",
        "           {prompt_context}\n",
        "           </context>\n",
        "           <question>\n",
        "           {myquestion}\n",
        "           </question>\n",
        "           Answer:\n",
        "           \"\"\"\n",
        "\n",
        "    json_data = json.loads(prompt_context)\n",
        "\n",
        "    relative_paths = set(item['relative_path'] for item in json_data['results'])\n",
        "\n",
        "    return prompt, relative_paths\n",
        "\n",
        "\n",
        "def answer_question(myquestion):\n",
        "\n",
        "    prompt, relative_paths =create_prompt (myquestion)\n",
        "\n",
        "    response = Complete(st.session_state.model_name, prompt)\n",
        "\n",
        "    return response, relative_paths\n",
        "\n",
        "def main():\n",
        "\n",
        "    st.title(f\":speech_balloon: Chat Document Assistant with Snowflake Cortex\")\n",
        "    st.write(\"This is the list of documents you already have and that will be used to answer your questions:\")\n",
        "    docs_available = session.sql(\"ls @docs\").collect()\n",
        "    list_docs = []\n",
        "    for doc in docs_available:\n",
        "        list_docs.append(doc[\"name\"])\n",
        "    st.dataframe(list_docs)\n",
        "\n",
        "    config_options()\n",
        "    init_messages()\n",
        "\n",
        "    # Display chat messages from history on app rerun\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    # Accept user input\n",
        "    if question := st.chat_input(\"What do you want to know about your products?\"):\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "        # Display user message in chat message container\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(question)\n",
        "        # Display assistant response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            message_placeholder = st.empty()\n",
        "\n",
        "            question = question.replace(\"'\",\"\")\n",
        "\n",
        "            with st.spinner(f\"{st.session_state.model_name} thinking...\"):\n",
        "                response, relative_paths = answer_question(question)\n",
        "                response = response.replace(\"'\", \"\")\n",
        "                message_placeholder.markdown(response)\n",
        "\n",
        "                if relative_paths != \"None\":\n",
        "                    with st.sidebar.expander(\"Related Documents\"):\n",
        "                        for path in relative_paths:\n",
        "                            cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{path}', 360) as URL_LINK from directory(@docs)\"\n",
        "                            df_url_link = session.sql(cmd2).to_pandas()\n",
        "                            url_link = df_url_link._get_value(0,'URL_LINK')\n",
        "\n",
        "                            display_url = f\"Doc: [{path}]({url_link})\"\n",
        "                            st.sidebar.markdown(display_url)\n",
        "\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}